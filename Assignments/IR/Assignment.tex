\documentclass[11pt]{article}
\pdfpagewidth 210mm
\pdfpageheight 297mm
\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9.0in}
\setlength\textwidth{6.0in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\usepackage{cite}
\usepackage{amsmath}

\newcommand{\sectionline}{%
  \nointerlineskip \vspace{\baselineskip}%
  \rule{0.5\linewidth}{.7pt}\hspace{\fill}%
  \par\nointerlineskip \vspace{\baselineskip}
}

\begin{document}
\title{Information Retrieval Assignment}
\author{Candidate 680817}
\maketitle

\section*{Question 1}
Converting queries to Disjunctive Normal Form is helpful since it allows
evaluation to be performed in one simple left-to-right pass.  It does not
necessarily reduce the number of evaluations that need to be performed --
indeed, it may increase the size of the formula, but the simpler form may allow
the calculations to be done more efficiently. For example, after normalization,
the disjuncts can be evaluated independently. 

\begin{align*}
& \quad (t_a \wedge \neg (t_b \wedge t_c)) \wedge (t_a \vee t_d)                                                   & \\
& \equiv (t_a \wedge (\neg t_b \vee \neg t_c)) \wedge (t_a \vee t_d)                                               & \text{by De Morgan} \\
& \equiv (t_a \wedge (\neg t_b \vee \neg t_c) \wedge t_a) \vee ( (t_a \wedge (\neg t_b \vee \neg t_c) \wedge t_d ) & \text{by distributivity} \\
& \equiv (t_a \wedge (\neg t_b \vee \neg t_c)) \vee ( (t_a \wedge (\neg t_b \vee \neg t_c) \wedge t_d ) & \text{as } \phi \wedge \phi \equiv \phi \\
& \equiv (t_a \wedge (\neg t_b \vee \neg t_c)) & \text{as } \phi \vee (\phi \wedge \psi) \equiv \phi \\
& \equiv (t_a \wedge \neg t_b) \vee (t_a \wedge \neg t_c) & \text{by distributivity} \\
\end{align*}

\section*{Question 2}
The problem of text-based information retrieval involves determining the
relevancy of documents to a given query. A natural approach is to situate both
queries and documents in a single `topic space', wherein some appropriate
measure of distance will then correspond to the desired indicator of relevancy:
points close together in the space are considered to be more relevant to
one another than points far apart. Perhaps surprisingly, given its simplicity,
the ordinary vector space proves to be a very effective choice for modelling
this. The key to this effectiveness is the use of a large number of dimensions
- one for every term which appears in the document set.

The components of a document or query vector in the space are then determined
by the weighting scheme. The TF-IDF weighting scheme is a standard way to do
this: the component of the document vector which corresponds to a given term is
calculated by multiplying the relevant \emph{term frequency} by the
\emph{inverse document frequency} for that term. The term frequency is the
number of times the term appears in the document. The inverse document
frequency is the logarithm of the reciprocal of the proportion of the documents
in the set which contain the relevant term. Consequently, a document
will have a large component for a term if that term appears frequently in that
document and is otherwise not very common in the document set. The position
vector of a document or query in the space is therefore determined by which
terms are most unique to it. With the standard Euclidean metric, documents
which share terms which are relatively unusual will then be closer in the
vector space.

This is our model of the notion of relevancy referred to above: we assume that
a document is more likely to be relevant to a query if they contain the same,
distinctive terms.  In fact, for comparing documents with queries, the
discrepant lengths mean it is sensible to normalize both vectors first. In this
case the cosine similarity measure is equivalent to the Euclidean distance, and
is more efficient to calculate, so it is typically used instead.

While the vector space model has been very fruitful as a basis of modern
information retrieval systems, some of the assumptions involved are
oversimplifications, and as such the model is not without its problems. For
example, in using terms as basis vectors for the space there is an implicit
assumption that these terms are in some sense semantically orthogonal. In cases
such as synonymy, this is far from true: `football' and `soccer' mean
essentially the same thing, so it would be expedient for this to be reflected
in the model. With the basic vector space model, a query containing one will
not return documents which contain only the other. Supplementary techniques
like query expansion can ameliorate these difficulties to some extent, but it
is nevertheless important to be aware of problems that can result from the
underlying `bag of words' assumption.

\section*{Question 3}

Inverted list:
\begin{verbatim}
{'document': [Doc1, Doc2, Doc3]),
 'file': [Doc1, Doc2, Doc3]),
 'index': [Doc1, Doc2, Doc3]),
 'inverted': [Doc1, Doc2, Doc3]),
 'list': [Doc1, Doc2, Doc3]),
 'postings': [Doc1, 'Doc3']),
 'retrieval': [Doc1, Doc2, Doc3]),
 'search': [Doc1, Doc2, Doc3]),
 'term': [Doc1, Doc2, Doc3]),
 'text': [Doc1, Doc2, Doc3]),
 'word': [Doc1, Doc2, Doc3])}
\end{verbatim}

Cosine similarity results:
\begin{verbatim}
doc:  IRExamDocs/Doc1.txt
query:  set(['list', 'retrieval', 'inverted'])
doc weight:  7.08081847238
query weight:  0.824561673745
cosine similarity: 0.198441745705 

doc:  IRExamDocs/Doc2.txt
query:  set(['list', 'retrieval', 'inverted'])
doc weight:  5.23758638986
query weight:  0.824561673745
cosine similarity: 0.221995400179 

doc:  IRExamDocs/Doc3.txt
query:  set(['list', 'retrieval', 'inverted'])
doc weight:  6.87233224281
query weight:  0.824561673745
cosine similarity: 0.252241450846
\end{verbatim}

See appendix for program used.

\section*{Question 4}

We evaluate performance by calculating precision at recall values because we
wish to be able to compare systems which make different trade-offs for
precision and recall against rank. In addition, taking a straight average of
precision at rank values gives a value which, since precision as a function of
rank is likely to be far from linear, will be affected strongly by the size of
the document set and the number of relevant documents. Choosing a fixed set of
recall values, calculating the precision at these, and taking the average of
the results eliminates those confounding variables, and is therefore a sensible
way of comparing the performance of different systems.

This table shows precision and recall calculations for the given data:

\begin{tabular}{c l c l l}
Rank & Document & Relevant & Recall & Precision \\
\hline
1 & $d_{77}$  & Y & 1/15 = 0.0666 & 1/1 \\
2 & $d_{19}$  & Y & 2/15 = 0.133  & 2/2 = 1.00 \\
3 & $d_{14}$  & N & 2/15          & 2/3 \\
4 & $d_{123}$ & Y & 3/15 = 0.200  & 3/4 = 0.75 \\
5 & $d_{92}$  & Y & 4/15 = 0.267  & 4/5 \\
6 & $d_{4}$   & N & 4/15          & 4/6 \\
7 & $d_{104}$ & Y & 5/15 = 0.333  & 5/7 = 0.714 \\
8 & $d_{36}$  & Y & 6/15 = 0.400  & 6/8 = 0.75 \\
9 & $d_{173}$ & N & 6/15          & 6/9 \\
10 & $d_{37}$ & N & 6/15          & 6/10 \\
\end{tabular}

So the precision @ recall values are:

\begin{tabular}{l l}
Recall & Precision \\
\hline
0.1 & 1.00 \\
0.2 & 0.75 \\
0.3 & 0.714 \\
0.4 & 0.75 \\
\end{tabular}

\section*{Question 5}

Initial means:

$\bar{a} = (0.9, 0.3)$

$\bar{b} = (0.6, 0.2)$

Calculate bisector, where $y = m \cdot x + c$:

$m = \frac{(a_x - b_x)}{(b_y - a_y)} = -3.0$

$c = \frac{((a_y+b_y) - m \cdot (a_x+b_x))}{2} = 2.5$

\begin{tabular}{c | c | c | c}
Doc. No. & $t_1$ & $t_2$ & $m \cdot t_1 + c - t_2$ \\
\hline
1 & 0.9 & 0.3 & -0.5 \\
2 & 0.6 & 0.2 & 0.5 \\
3 & 0.1 & 0.8 & 1.4 \\
4 & 0.3 & 0.9 & 0.7 \\
5 & 0.2 & 0.7 & 1.2 \\
6 & 0.1 & 0.8 & 1.4 \\
7 & 0.7 & 0.3 & 0.1 \\
8 & 0.8 & 0.3 & -0.2 \\
\end{tabular}

Clusters:

$A = \{1,8\}$
$B = \{2,3,4,5,6,7\}$

\sectionline

Means:

$\bar{a} = (0.85, 0.3)$

$\bar{b} = (0.333333333333, 0.616666666667)$

Calculate bisector, where $y = m \cdot x + c$:

$m = \frac{(a_x - b_x)}{(b_y - a_y)} = 1.63157894737$

$c = \frac{((a_y+b_y) - m \cdot (a_x+b_x))}{2} = -0.50701754386$

\begin{tabular}{c | c | c | c}
Doc. No. & $t_1$ & $t_2$ & $m \cdot t_1 + c - t_2$ \\
\hline
1 & 0.9 & 0.3 & 0.661403508772 \\
2 & 0.6 & 0.2 & 0.271929824561 \\
3 & 0.1 & 0.8 & -1.14385964912 \\
4 & 0.3 & 0.9 & -0.917543859649 \\
5 & 0.2 & 0.7 & -0.880701754386 \\
6 & 0.1 & 0.8 & -1.14385964912 \\
7 & 0.7 & 0.3 & 0.335087719298 \\
8 & 0.8 & 0.3 & 0.498245614035 \\
\end{tabular}

Clusters:

$A = \{3,4,5,6\}$
$B = \{1,2,7,8\}$

\sectionline

Means:

$\bar{a} = (0.175, 0.8)$

$\bar{b} = (0.75, 0.275)$

Calculate bisector, where $y = m \cdot x + c$:

$m = \frac{(a_x - b_x)}{(b_y - a_y)} = 1.09523809524$

$c = \frac{((a_y+b_y) - m \cdot (a_x+b_x))}{2} = 0.0309523809524$

\begin{tabular}{c | c | c | c}
Doc. No. & $t_1$ & $t_2$ & $m \cdot t_1 + c - t_2$ \\
\hline
1 & 0.9 & 0.3 & 0.716666666667 \\
2 & 0.6 & 0.2 & 0.488095238095 \\
3 & 0.1 & 0.8 & -0.659523809524 \\
4 & 0.3 & 0.9 & -0.540476190476 \\
5 & 0.2 & 0.7 & -0.45 \\
6 & 0.1 & 0.8 & -0.659523809524 \\
7 & 0.7 & 0.3 & 0.497619047619 \\
8 & 0.8 & 0.3 & 0.607142857143 \\
\end{tabular}

Clusters:

$A = \{3,4,5,6\}$
$B = \{1,2,7,8\}$

\sectionline

The process has converged, so this is the final result.

\section*{Question 6}

Initial data, $X$:

\begin{tabular}{c c c c c c c}
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
0 & 1 & 0 & 1 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 1 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
\end{tabular}

After applying an erosion:

\begin{tabular}{c c c c c c c}
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{tabular}

Final result, after applying a dilation to the previous output:

\begin{tabular}{c c c c c c c}
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 1 & 0 \\
0 & 0 & 1 & 1 & 1 & 0 & 0 \\
\end{tabular}

As can be seen from these results, the opening transformation removes any
isolated `handles' which are not strongly connected to the core of the object.
In this case, the pixels at coordinates (2,1) and (2,5) are set to zero because
there are no non-zero pixels horizontally or vertically adjacent to them.

\section*{Appendix}

Cosine similarity program:
\footnotesize
\begin{verbatim}
#!/usr/bin/python2
import glob
import re
import pprint
import math

class Corpus(object):
   def __init__(self, file_list, vocab):
      self._file_list = file_list
      self._N = 0;
      self._inverted_index = dict()
      self._vocab = vocab
      self._docs = dict()
      self._doc_freqs = {
         'document': 15,
         'file': 24,
         'index': 15,
         'inverted': 22,
         'list': 19,
         'postings': 13,
         'retrieval': 6,
         'search': 18,
         'term': 17,
         'text': 6,
         'word': 5
      }
      self._freqs = dict()
   
   def _process_word(self, file_name, word):
      if word in self._vocab:
         if word not in self._inverted_index:
            self._inverted_index[word] = set()
         self._inverted_index[word].add(file_name)
         if file_name not in self._docs:
            self._docs[file_name] = set()
            self._freqs[file_name] = dict()
            self._N += 1
         self._docs[file_name].add(word)
         if word not in self._freqs[file_name]:
            self._freqs[file_name][word] = 0
         self._freqs[file_name][word] += 1


   def _process_line(self, file_name, line):
      for word in re.split('\W', line):
         self._process_word(file_name, word)

   def _process_file(self, file_name):
      fs = open(file_name, 'r')
      for line in fs:
         self._process_line(file_name, line)
      fs.close()

   def index(self):
      for file_name in self._file_list:
         self._process_file(file_name)

   def __repr__(self):
      return "%s" % pprint.pformat(self._inverted_index)

   # The lengths could be stored, but doesn't matter here
   def doc_length(self, doc_name, doc):
      total = 0;
      for term in doc:
         freq_d_t = float(self._freqs[doc_name][term])
         total += (1 + math.log(freq_d_t))**2
      return math.sqrt(total)

   def query_length(self, query):
      total = 0;
      for term in query:
         freq_t = float(self._doc_freqs[term])
         total += math.log(1 + self._N/freq_t)
      return math.sqrt(total)

   def cosine(self, query, doc_name):
      doc = self._docs[doc_name]
      print "doc: ", doc_name
      print "query: ", query 
      w_doc   = self.doc_length(doc_name, doc)
      w_query = self.query_length(query)
      print "doc weight: ", w_doc
      print "query weight: ", w_query
      total = 0
      for term in query.intersection(doc):
         freq_d_t = float(self._freqs[doc_name][term])
         freq_t   = float(self._doc_freqs[term])
         total += (1 + math.log(freq_d_t)) * math.log(1 + self._N/freq_t)
      return total / (w_query * w_doc)

def main():
   vocab = {'document', 'file', 'index', 'inverted', 'list', 'postings',
            'retrieval', 'search', 'term', 'text', 'word'}
   docs = glob.glob("IRExamDocs/Doc?.txt")
   cs = Corpus(docs, vocab)
   cs.index()

   query = {'inverted', 'list', 'retrieval'}
   for d in docs:
      res = cs.cosine(query, d)
      print "cosine similarity:", res, "\n"

main()
\end{verbatim}

\end{document}

